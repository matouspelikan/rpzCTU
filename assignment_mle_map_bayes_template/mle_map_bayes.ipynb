{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Maximum likelihood, Maximum a posteriori and Bayesian inference estimates of distribution parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from scipy.stats import norm\n",
    "\n",
    "import mle_map_bayes as mmb\n",
    "\n",
    "from plotting_functions import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# generate data\n",
    "num_samples = \n",
    "mu_true = \n",
    "var_true = \n",
    "sigma_true = np.sqrt(var_true)\n",
    "x = mu_true + np.random.randn(num_samples) * sigma_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# ML estimate\n",
    "mu_mle, var_mle = mmb.ml_estim_normal(x)\n",
    "\n",
    "print('estimated mean: {:.5}'.format(mu_mle))\n",
    "print('estimated sigma: {:.5}'.format(np.sqrt(var_mle)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# plot the likelihood\n",
    "# NOTICE: this works only for small num_samples (<=500), otherwise one gets zero everywhere due to numerical errors!\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_likelihood(x, -1.0, 1.0, 0, 2)\n",
    "plt.plot([mu_true], [var_true], 'r+', markeredgewidth=2, markersize=10)\n",
    "plt.plot([mu_mle], [var_mle], 'kx', markeredgewidth=2, markersize=10)\n",
    "plt.legend(['true', 'MLE'])\n",
    "\n",
    "# plot the estimated and true distributions\n",
    "plt.subplot(1, 2, 2)\n",
    "span_x = sigma_true * 5\n",
    "z = np.linspace(mu_true - span_x, mu_true + span_x, 100)\n",
    "p_true = norm.pdf(z, mu_true, sigma_true)\n",
    "p_mle = norm.pdf(z, mu_mle, np.sqrt(var_mle))\n",
    "plt.plot(z, p_true, 'r-')\n",
    "plt.plot(z, p_mle, 'k-')\n",
    "plt.plot(x, [-0.001] * x.size, 'ko')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p(x)')\n",
    "plt.xlim([mu_true - span_x, mu_true + span_x])\n",
    "plt.legend(['true pdf', 'MLE pdf', 'data'])\n",
    "plt.savefig('mle_normal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated distributions with increasing number of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, sharey='row', figsize=(15, 5))\n",
    "\n",
    "span_x = sigma_true * 5\n",
    "z = np.linspace(mu_true - span_x, mu_true + span_x, 100)\n",
    "p_true = norm.pdf(z, mu_true, sigma_true)\n",
    "\n",
    "for (plot_pos, num) in zip((0, 1, 2), (2, 5, 50)):\n",
    "    data = x[:num]\n",
    "    mu_mle_part, var_mle_part = mmb.ml_estim_normal(data)\n",
    "    \n",
    "    # plot the estimated and true distributions\n",
    "    plt.sca(axes[plot_pos])\n",
    "    p_mle = norm.pdf(z, mu_mle_part, np.sqrt(var_mle_part))\n",
    "    plt.plot(z, p_true, 'r-')\n",
    "    plt.plot(z, p_mle, 'k-')\n",
    "    plt.plot(data, [-0.001] * data.size, 'ko')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('p(x)')\n",
    "    plt.xlim([mu_true - span_x, mu_true + span_x])\n",
    "    plt.legend(['true', 'MLE', 'data'])\n",
    "\n",
    "plt.savefig('mle_normal_varying_dataset_size.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP prior settings\n",
    "mu0 = \n",
    "nu = \n",
    "alpha = \n",
    "beta = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution using the provided `plot_prior` function together with its mode \\[`mu_prior`, `var_prior`\\].\n",
    "\n",
    "Experiment with different settings of $\\mu_0, \\nu, \\alpha, \\beta$ to get the feeling for how each of the parameters influences the prior. You will need this understanding later when we will be building a classifier using the MAP estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the values of the peak of the prior distribution\n",
    "mu_prior = \n",
    "var_prior = \n",
    "\n",
    "# plot the prior\n",
    "plot_margin = 2\n",
    "plt.figure(figsize=(6, 5))\n",
    "prior_grid = plot_prior(mu0, nu, alpha, beta, mu0 - plot_margin, mu0 + plot_margin, 0, 3)\n",
    "\n",
    "plt.plot([mu_prior], [var_prior], 'mx', markeredgewidth=2, markersize=10)\n",
    "plt.legend(['prior max'])\n",
    "plt.savefig('map_prior_normal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP estimate\n",
    "mu_map, var_map = mmb.map_estim_normal(x, mu0, nu, alpha, beta)\n",
    "\n",
    "print('estimated mean: {:.5}'.format(mu_map))\n",
    "print('estimated sigma: {:.5}'.format(np.sqrt(var_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the likelihood, prior and MAP objective together with the MLE and MAP estimates and the prior maximum.\n",
    "mu_min = -1\n",
    "mu_max = 1\n",
    "var_min = 0\n",
    "var_max = 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, sharey='row', figsize=(15, 5))\n",
    "plt.sca(axes[0])\n",
    "plot_likelihood(x, mu_min, mu_max, var_min, var_max)\n",
    "plt.plot([mu_mle], [var_mle], 'kx', markeredgewidth=2, markersize=10)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_prior(mu0, nu, alpha, beta, mu_min, mu_max, var_min, var_max)\n",
    "plt.plot([mu_prior], [var_prior], 'mx', markeredgewidth=2, markersize=10)\n",
    "\n",
    "# The following plot is NOT a probability distribution!!! It is not normalised to sum up to one!\n",
    "plt.sca(axes[2])\n",
    "plot_MAP_objective(x, mu0, nu, alpha, beta, mu_min, mu_max, var_min, var_max)\n",
    "plt.plot([mu_mle], [var_mle], 'kx', markeredgewidth=2, markersize=10)\n",
    "plt.plot([mu_prior], [var_prior], 'mx', markeredgewidth=2, markersize=10)\n",
    "plt.plot([mu_map], [var_map], 'gx', markeredgewidth=2, markersize=10)\n",
    "plt.legend(['MLE', 'prior', 'MAP'])\n",
    "plt.savefig('mle_map_prior_comparison_normal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the MLE and MAP estimates for different dataset sizes (e.g. 1, 5, 50):\n",
    "#   - plot the MAP objective with the estimates and prior indicated,\n",
    "#   - plot also the estimated distributions for each dataset size.\n",
    "\n",
    "mu_min = -1\n",
    "mu_max = 1\n",
    "var_min = 0\n",
    "var_max = 2\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, sharey='row', figsize=(15,10))\n",
    "for (plot_pos, num) in zip((0, 1, 2), (1, 5, 50)):\n",
    "    data = x[:num]\n",
    "    mu_map_part, var_map_part = mmb.map_estim_normal(data, mu0=mu0, nu=nu, alpha=alpha, beta=beta)\n",
    "    mu_mle_part, var_mle_part = mmb.ml_estim_normal(data)\n",
    "    \n",
    "    plt.sca(axes[0, plot_pos])\n",
    "    plot_MAP_objective(data, mu0, nu, alpha, beta, mu_min, mu_max, var_min, var_max)\n",
    "    if num > 1:\n",
    "        plt.plot([mu_mle_part], [var_mle_part], 'kx', markeredgewidth=2, markersize=10)\n",
    "    plt.plot([mu_prior], [var_prior], 'mx', markeredgewidth=2, markersize=10)\n",
    "    plt.plot([mu_map_part], [var_map_part], 'gx', markeredgewidth=2, markersize=10)\n",
    "    plt.xlabel('mu')\n",
    "    plt.ylabel('var')\n",
    "    plt.legend(['MLE', 'prior', 'MAP'])\n",
    "    plt.title('{:d} datapoint{:s}'.format(num, '' if num == 1 else 's'))\n",
    "    \n",
    "    # plot the estimated and true distributions\n",
    "    plt.sca(axes[1, plot_pos])\n",
    "    span_x = sigma_true * 5\n",
    "    z = np.linspace(mu_true - span_x, mu_true + span_x, 100)\n",
    "    p_true = norm.pdf(z, mu_true, sigma_true)\n",
    "    if var_mle_part > 0:\n",
    "        p_mle = norm.pdf(z, mu_mle_part, np.sqrt(var_mle_part))\n",
    "    p_map = norm.pdf(z, mu_map_part, np.sqrt(var_map_part))\n",
    "    plt.plot(z, p_true, 'r-')\n",
    "    if var_mle_part > 0:\n",
    "        plt.plot(z, p_mle, 'k-')\n",
    "    plt.plot(z, p_map, 'g-')\n",
    "    plt.plot(data, [-0.001] * data.size, 'ko')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('p(x)')\n",
    "    plt.xlim([mu_true - span_x, mu_true + span_x])\n",
    "    if var_mle_part > 0:\n",
    "        leg = plt.legend(['true', 'MLE', 'MAP', 'data'])\n",
    "    else:\n",
    "        leg = plt.legend(['true', 'MAP', 'data'])\n",
    "        \n",
    "plt.savefig('mle_map_normal_dataset_sizes.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the posterior probability and mark the MLE, MAP and maximum prior solutions\n",
    "\n",
    "# assumes the same data and parameters as for MAP estimate\n",
    "# assumes the MLE and MAP estimates are already computed\n",
    "\n",
    "plot_margin = 1\n",
    "plt.figure(figsize=(6, 5))\n",
    "prior_grid = plot_posterior_normal(x, mu0, nu, alpha, beta, mu0 - plot_margin, mu0 + plot_margin, 0, 2)\n",
    "plt.plot([mu_mle], [var_mle], 'kx', markeredgewidth=2, markersize=10)\n",
    "plt.plot([mu_prior], [var_prior], 'mx', markeredgewidth=2, markersize=10)\n",
    "plt.plot([mu_map], [var_map], 'gx', markeredgewidth=2, markersize=10)\n",
    "plt.legend(['MLE', 'prior', 'MAP'])\n",
    "plt.savefig('bayes_posterior_normal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictive distribution and its comparison with MAP and MLE\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, sharey='row', figsize=(15,3))\n",
    "\n",
    "span_x = sigma_true * 5\n",
    "z = np.linspace(mu_true - span_x, mu_true + span_x, 100)\n",
    "p_true = norm.pdf(z, mu_true, sigma_true)\n",
    "\n",
    "for (plot_pos, num) in zip((0, 1, 2), (1, 5, 15)):\n",
    "    data = x[:num]\n",
    "    mu_map_part, var_map_part = mmb.map_estim_normal(data, mu0=mu0, nu=nu, alpha=alpha, beta=beta)\n",
    "    mu_mle_part, var_mle_part = mmb.ml_estim_normal(data)\n",
    "    \n",
    "    # plot the estimated and true distributions\n",
    "    if var_mle_part > 0:\n",
    "        p_mle = norm.pdf(z, mu_mle_part, np.sqrt(var_mle_part))\n",
    "    p_map = norm.pdf(z, mu_map_part, np.sqrt(var_map_part))\n",
    "    p_bayes = mmb.bayes_estim_pdf_normal(z, data, mu0, nu, alpha, beta)\n",
    "    \n",
    "    plt.sca(axes[plot_pos])\n",
    "    plt.plot(z, p_true, 'r-')\n",
    "    if var_mle_part > 0:\n",
    "        plt.plot(z, p_mle, 'k-')\n",
    "    plt.plot(z, p_map, 'g-')\n",
    "    plt.plot(z, p_bayes, 'b-')\n",
    "    plt.plot(data, [-0.001] * data.size, 'ko')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('p(x)')\n",
    "    if var_mle_part > 0:\n",
    "        leg = plt.legend(['true', 'MLE', 'MAP', 'Bayes', 'data'])\n",
    "    else:\n",
    "        leg = plt.legend(['true', 'MAP', 'Bayes', 'data'])\n",
    "    plt.xlim([mu_true - span_x, mu_true + span_x])\n",
    "    plt.title('{:d} datapoint{:s}'.format(num, '' if num == 1 else 's'))\n",
    "\n",
    "plt.savefig('mle_map_bayes_normal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some outlier to the data (point not from the distribution)\n",
    "x_noise = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the estimated and true distributions\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "span_x = sigma_true * 5\n",
    "z = np.linspace(mu_true - span_x, 9, 100)\n",
    "\n",
    "mu_mle_noise, var_mle_noise = mmb.ml_estim_normal(x_noise)\n",
    "mu_map_noise, var_map_noise = mmb.map_estim_normal(x_noise, mu0, nu, alpha, beta)\n",
    "\n",
    "p_true = norm.pdf(z, mu_true, sigma_true)\n",
    "p_mle = norm.pdf(z, mu_mle_noise, np.sqrt(var_mle_noise))\n",
    "p_map = norm.pdf(z, mu_map_part, np.sqrt(var_map_noise))\n",
    "p_bayes = mmb.bayes_estim_pdf_normal(z, data, mu0, nu, alpha, beta)\n",
    "plt.plot(z, p_true, 'r-')\n",
    "plt.plot(z, p_mle, 'k-')\n",
    "plt.plot(z, p_map, 'g-')\n",
    "plt.plot(z, p_bayes, 'b-')\n",
    "plt.plot(x_noise, [-0.001] * x_noise.size, 'ko')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p(x)')\n",
    "plt.xlim([mu_true - span_x, 9])\n",
    "leg = plt.legend(['true pdf', 'MLE pdf', 'MAP pdf', 'Bayes pdf', 'data'])\n",
    "\n",
    "plt.savefig('noise.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Categorical distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some data\n",
    "num_classes = 6\n",
    "num_samples = 50\n",
    "pk_true = \n",
    "counts = np.random.multinomial(num_samples, pk_true)\n",
    "\n",
    "# visualise the data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plot_categorical_distr(pk_true, 'true_distribution')\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_categorical_hist(counts, 'training data histogram')\n",
    "\n",
    "plt.savefig('categorical_data.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the estimate\n",
    "pc_mle = mmb.ml_estim_categorical(counts)\n",
    "\n",
    "# visualise the estimate\n",
    "plot_categorical_distr(pc_mle, 'MLE estimate')\n",
    "\n",
    "plt.savefig('mle_categorical.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random samples from the conjugate prior\n",
    "\n",
    "alphas = # in this case of a \"flat\" prior, the solution reverts back to MLE\n",
    "prior_samples = np.random.dirichlet(alphas, 5)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 3), sharey='row')\n",
    "for i in range(5):\n",
    "    plt.sca(axes[i])\n",
    "    plot_categorical_distr(prior_samples[i], '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the MAP estimate\n",
    "pk_map = mmb.map_estim_categorical(counts, alphas)\n",
    "\n",
    "plot_categorical_distr(pk_map, 'MAP estimate')\n",
    "plt.savefig('map_categorical.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the posterior\n",
    "pc_posterior = mmb.bayes_posterior_params_categorical(counts, alphas)\n",
    "\n",
    "# random samples from the posterior p(pc_posterior|counts)\n",
    "posterior_samples = np.random.dirichlet(pc_posterior, 5)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 3), sharey='row')\n",
    "for i in range(5):\n",
    "    plt.sca(axes[i])\n",
    "    plot_categorical_distr(posterior_samples[i], '')\n",
    "    \n",
    "plt.savefig('bayes_posterior_categorical.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_bayes = mmb.bayes_estim_categorical(counts, alphas)\n",
    "\n",
    "plot_categorical_distr(pc_bayes, 'Bayesian estimate')\n",
    "plt.savefig('bayes_categorical.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "loaded_data = np.load(\"data_33rpz_mle_map_bayes.npz\", allow_pickle=True)\n",
    "\n",
    "alphabet = loaded_data[\"alphabet\"]\n",
    "\n",
    "tst = loaded_data[\"tst\"].item()\n",
    "\n",
    "trn_20 = loaded_data[\"trn_20\"].item()\n",
    "trn_200 = loaded_data[\"trn_200\"].item()\n",
    "trn_2000 = loaded_data[\"trn_2000\"].item()\n",
    "\n",
    "trn_sets = {'20': trn_20, '200': trn_200, '2000': trn_2000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the data using all three estimates\n",
    "\n",
    "# select the training set\n",
    "picked_set = '20' # your code probably won't work for '200' or '2000' due to numerical limitations\n",
    "# feel free to search for the source of the numerical problems and come up with a workaround\n",
    "# (it is doable relatively easily for '200')\n",
    "trn_set = trn_sets[picked_set]\n",
    "\n",
    "# computing LR feature vectors (training set)\n",
    "x_train = \n",
    "labels_train = trn_set['labels']\n",
    "\n",
    "# Splitting the trainning data into into classes\n",
    "x_A = x_train[labels_train == 0]\n",
    "x_C = x_train[labels_train == 1]\n",
    "\n",
    "# NIG prior settings\n",
    "mu0_A, nu_A, alpha_A, beta_A = \n",
    "mu0_C, nu_C, alpha_C, beta_C = \n",
    "\n",
    "q_mle, labels_mle, DA_mle, DC_mle = mmb.mle_Bayes_classif(tst['images'], x_A, x_C)\n",
    "error_mle = mmb.classification_error_2normal(tst['images'], tst['labels'], q_mle)\n",
    "print('MLE classification error: {:.2f} %'.format(error_mle * 100))\n",
    "\n",
    "q_map, labels_map, DA_map, DC_map = mmb.map_Bayes_classif(tst['images'], \n",
    "                                                                  x_A, x_C,\n",
    "                                                                  mu0_A, nu_A, alpha_A, beta_A, \n",
    "                                                                  mu0_C, nu_C, alpha_C, beta_C)\n",
    "error_map = mmb.classification_error_2normal(tst['images'], tst['labels'], q_map)\n",
    "print('MAP classification error: {:.2f} %'.format(error_map * 100))\n",
    "\n",
    "x_test = mmb.compute_measurement_lr_cont(tst['images'])\n",
    "labels_map = mmb.bayes_Bayes_classif(x_test, x_A, x_C,\n",
    "                                     mu0_A, nu_A, alpha_A, beta_A, \n",
    "                                     mu0_C, nu_C, alpha_C, beta_C)\n",
    "error_bayes = np.sum(tst['labels'] != labels_map) / labels_map.size\n",
    "print('Bayes classification error: {:.2f} %'.format(error_bayes * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the estimates\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "x_tst_all = mmb.compute_measurement_lr_cont(tst['images'])\n",
    "x_A_tst = x_tst_all[tst['labels'] == 0]\n",
    "x_C_tst = x_tst_all[tst['labels'] == 1]\n",
    "plt.hist(x_A_tst, 20, density=True)\n",
    "plt.hist(x_C_tst, 20, density=True, alpha=0.8)\n",
    "z = np.linspace(-4000, 3000, 1000)\n",
    "\n",
    "p_mle_A = norm.pdf(z, DA_mle['Mean'], DA_mle['Sigma'])\n",
    "p_mle_C = norm.pdf(z, DC_mle['Mean'], DC_mle['Sigma'])\n",
    "plt.plot(z, p_mle_A, 'b:', label='MLE pdf A')\n",
    "plt.plot(z, p_mle_C, 'r:', label='MLE pdf C')\n",
    "\n",
    "p_map_A = norm.pdf(z, DA_map['Mean'], DA_map['Sigma'])\n",
    "p_map_C = norm.pdf(z, DC_map['Mean'], DC_map['Sigma'])\n",
    "plt.plot(z, p_map_A, 'b--', label='MAP pdf A')\n",
    "plt.plot(z, p_map_C, 'r--', label='MAP pdf C')\n",
    "\n",
    "p_bayes_A = mmb.bayes_estim_pdf_normal(z, x_A, mu0_A, nu_A, alpha_A, beta_A)\n",
    "p_bayes_C = mmb.bayes_estim_pdf_normal(z, x_C, mu0_C, nu_C, alpha_C, beta_C)\n",
    "plt.plot(z, p_bayes_A, 'b-', label='Bayes pdf A')\n",
    "plt.plot(z, p_bayes_C, 'r-', label='Bayes pdf C')\n",
    "\n",
    "plt.plot([q_mle['t1'], q_mle['t1']], [0, 0.001], 'k:', label='MLE strategy')\n",
    "plt.plot([q_mle['t2'], q_mle['t2']], [0, 0.001], 'k:', label=None)\n",
    "plt.plot([q_map['t1'], q_map['t1']], [0, 0.001], 'k--', label='MAP strategy')\n",
    "plt.plot([q_map['t2'], q_map['t2']], [0, 0.001], 'k--', label=None)\n",
    "\n",
    "classif_bayes = mmb.bayes_Bayes_classif(z, x_A, x_C,\n",
    "                                        mu0_A, nu_A, alpha_A, beta_A,\n",
    "                                        mu0_C, nu_C, alpha_C, beta_C)\n",
    "y_val = np.ones_like(z) * (-0.00005)\n",
    "plt.plot(z[classif_bayes == 1], y_val[classif_bayes == 1], 'r.', label='Bayes_strategy_A')\n",
    "plt.plot(z[classif_bayes == 0], y_val[classif_bayes == 0], 'b.', label='Bayes_strategy_C')\n",
    "\n",
    "plt.xlim([-4000, 3000])\n",
    "plt.legend()\n",
    "plt.xlabel('measurement')\n",
    "plt.savefig('mle_map_bayes_Bayes_classifier.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
